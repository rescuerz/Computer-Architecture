# 五. DLP and TLP

[TOC]

![](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643958.png)

## 5.1 SIMD: vector processor

* SIMD architectures can exploit significant **`data-level parallelism`**
    * Matrix-oriented scientific computing
    * Media-oriented image and sound processors

* SIMD is **`more energy efficient`** than MIMD

    SIMD 比 MIMD 更节能

* SIMD allows programmer to continue to think sequentially

> **SIMD（单指令多数据）架构** 能够利用显著的 **数据级并行性**，适用于矩阵运算和多媒体处理。与 MIMD（多指令多数据）相比，SIMD **更节能**，并且允许程序员继续以顺序思维编程。

### 5.1.1 Vector Processor & Scalar Processor

* A pipeline processor, in which the **`vector data representation`** and the **`corresponding vector instructions`** are set, is called the **vector processor**.

  设置了 **`矢量数据表示和相应矢量指令`** 的流水线处理器称为矢量处理器。

* A pipeline processor that does not have vector data representation and corresponding vector instructions is called a **scalar processor**.

  **`没有矢量数据表示和相应矢量指令`** 的流水线处理器称为标量处理器。

通常有三种处理模式：

* **`Horizontal processing method 横向处理`**
  
    * Vector calculations are performed horizontally from left to right in a row.
    
        横向计算，从左到右，逐个计算出后再进行下一行。
    
    * Problems with horizontal processing:
        * When calculating each component, *RAW* correlation occurs, and the *pipeline efficiency is low*.
        
            **每次计算组件时，会发生 `RAW相关性`，导致流水线效率低。**
        
        * If a static multi-functional pipeline is used, the pipeline must be switched frequently; the throughput  of the pipeline is lower than that of sequential serial execution.
        
            **如果使用静态多功能流水线，流水线必须频繁切换，吞吐量低于顺序串行执行。**
        
        * **The horizontal processing method is not suitable for vector processors.**
        
            **`横向处理方法不适合向量处理器`。**
    
* **`Vertical processing method 纵向处理方法`**

    The vector calculation is performed vertically **from top to bottom** in a column manner.

* Horizontal and vertical processing method(**`group processing method`**)

    **combination**

> !!! Example
>
> D = A $\times$ (B + C) A, B, C, D ── vector of length N
>
> * Horizontal processing method
>
>     <center>
>         <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301707710.png" alt="image-20241130170724663" width=80%/>
>     </center>
>
>     先计算 $d_1\leftarrow a_1\times(b_1 +c_1)$，再计算 $d_2\leftarrow a_2\times(b_2 +c_2)$，以此类推。写作循环可以写作：
>
>     $$
>     \begin{aligned}
>     k_i & \leftarrow b_i+c_i\\
>     d_i & \leftarrow a_i \times k_i
>     \end{aligned}
>     $$
>
>     **`循环里的两个语句存在数据相关。因此有 N 个数据相关，需要进行 2N 次功能切换(加法和乘法)。`**
>
> * Vertical processing method
>
>     <center>
>         <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301710753.png" alt="image-20241130171001713" />
>     </center>
>
>     先计算加法，B+C 得到一个向量 K，再计算乘法，$A \times K$ 得到 D。
>
>     $$
>     \begin{aligned}
>     K & \leftarrow B + C\\
>     D& \leftarrow A\times K
>     \end{aligned}
>     $$
>
>     **`这里只有 1 个数据相关，2 次功能切换。`**
>
>     Requirements for processor structure: **`memory-memory structure`**.  
>
>     源向量和目的向量都在内存中，先取出来放到 buffer 中，进入 pipeline 进行计算后得到中间结果，中间结果先写到 buffer，最后写回内存。
>
>     <div align = center> <img src="https://cdn.hobbitqia.cc/20231215195250.png" width=60%> </div>
>
> * Horizontal and vertical processing method
>
>     <center>
>         <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301714584.png" alt="image-20241130171417540" width=80%/>
>     </center>
>     
>     如果 N 太大了，我们不能用一个向量来直接装下所有数据，就需要多次运算。
>     
>     假设 $N=S\times n +r$，即我们把 N 个数据分成了 S 组，每组有 n 个数据，最后一组有 r 个数据。组内做纵向运算，组间做横向运算。
>     
>     Requirements for processor structure: **`register-register structure`**.
>     
>     **使用向量寄存器保存源向量，目标向量，中间结果。因为划分为多个组小份，寄存器装得下，访问寄存器的速度明显快于访问内存**  





### 5.1.2 Vector Processor Example - Cray-1

<div align = center> <img src="https://cdn.hobbitqia.cc/20231215195351.png" width=100%> </div>

有 8 个向量寄存器，每组向量寄存器有 64 位。**`有 12 条单功能流水线，可以并行工作`**。
<div align = center> <img src="https://cdn.hobbitqia.cc/20231215195456.png" width=70%> </div>

- Each vector register Vi has a separate bus connected to 6 vector functional units.

  每个向量寄存器 Vi 都有独立的总线连接到 6 个向量功能单元。

- Each vector function unit also has a bus that returns the result of the operation to the vector register bus.

  每个向量功能单元也有一条独立的总线返回运算结果到向量寄存器

- 不同的功能需要的拍数不同。

**`向量的指令也是可能存在相关性,只要没有Vi conflict 和 functional conflict，每一个Vi和functional unit 就能并行工作，提高向量运算的速度：`**  

* **Vi conflict**: The source vector or result vector of each vector instruction working in parallel **`uses the same Vi.`**

    **`当向量寄存器有依赖的时候，后续指令要在前面指令的结果出来之后再执行。`**

    > **这里并不是等前面的向量的每一个元素都计算完，而是 `等前面的向量的第一个元素计算完就开始计算第一个元素的后续指令，等第二个元素计算完就开始计算第二个元素的后续指令`，以此类推。**

    * Writing and reading data related

        $$
        \begin{aligned}
        V0 & \leftarrow V1+V2\\
        V3& \leftarrow V0\times V4
        \end{aligned}
        $$
        **写后读相关**：这里 V0 的计算结果需要先完成，才能用于 V3 的计算

    * Reading data related

        $$
        \begin{aligned}
        V0 & \leftarrow V1+V2\\
        V3& \leftarrow V1\times V4
        \end{aligned}
        $$
        **读后读相关：**

* **Functional conflict**: Each vector instruction working in parallel must **`use the same functional unit`**.
    $$
    \begin{aligned}
    V3 & \leftarrow V1\times V2\\
    V5& \leftarrow V4\times V6
    \end{aligned}
    $$
    
    如果我们只有一个乘法部件，就会有 **`结构冲突`**。
    
    **`我们只能等前一条指令全部完成（最后一个元素做完才可以），才能开始下一条指令。`**

#### Instruction Types of CRAY-1

<div align = center> <img src="https://cdn.hobbitqia.cc/20231215200227.png" width=70%> </div>

<center>
    <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301734223.png" alt="image-20241130173407182" width=80%/>
</center>



- 向量和向量运算，加法需要 6 拍；乘法需要 7 拍
- 向量和标量运算，加法需要 6 拍；乘法需要 7 拍
- 读写操作，内存和向量寄存器交互，读写需要 6 拍。



#### Improve the Performance of Vector Processor

* Set up multiple functional units and make them work in parallel.

  **设置多个功能单元并使其并行工作**：通过增加功能单元的数量，可以同时处理更多的向量操作

* Use **link technology** to speed up the execution of a string of vector instructions.

  **使用链接技术加速向量指令串的执行**
  $$
  \begin{aligned}
  K & \leftarrow B + C\\
  D& \leftarrow A\times K
  \end{aligned}
  $$
  **`例如此处计算完bi + ci之后，就可以计算ai * ki，独立功能单元，不需要等待B + C完全执行结束才执行乘法`**

* Adopt **recycling mining technology** to speed up recycling processing.

  **采用循环挖掘技术加速循环处理**：通过优化循环结构

* Using a **multi-processor system** to further improve the performance.

**`这里 1、3、4 都依赖于增加部件，我们将关注于第 2 种方法。`**

Link feature: It has two related instructions that are **`written first and then read`**. 

In the case of no conflicts between functional components and source vector conflicts, functional components can be linked for pipeline processing to achieve the purpose of speeding up execution.  

链接技术主要用于加速向量指令串的执行。当两条指令之间存在写后读相关性时(如果我们有两条指令，第一条指令的结果是第二条指令的输入)，可以将它们链接起来，从而减少一次读写操作的时间。

> !!! Example "Use link technology to perform vector operations on CRAY-1"
>
> D = A $\times$ (B + C) A, B, C, D ── vector of length N
>
> 假设 $N\leq 64$，均为浮点数，B 和 C 已经被存在 V0 和 V1 中。
>
> ```  asm
> V3 <- memory    // access vector A
> V2 <- V0 + V1  	// Vector B and Vector C perform floating point addition
> V4 <- V2 * V3   // Floating point multiplication, the result is stored in V4
> ```
> <center>
>     <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301746957.png" alt="image-20241130174635920" />
> </center>
>
> 这里前两条指令没有冲突，可以并行完成。第三条指令需要等前两条指令完成，存在 RAW，不能并行但可以链接。
>
> **`这里假设把数据从寄存器送到功能部件需要一拍，功能部件的结果写回到寄存器也需要一拍。把数据从内存送到 fetch function unit 需要一拍。`**
>
> <div align = center> <img src="https://cdn.hobbitqia.cc/20231215201205.png" width=70%> </div>
>
>     ``` asm
>     V3 <- A
>     V2 <- V0 + V1
>     V4 <- V2 * V3
>     ```
> ​    注意到向量功能内部也是流水的。
>
> * The execution time using serial method. 
>
>     **串行执行**：
>
>     - 第一条指令：`V3 <- memory` 需要 1 拍从内存读取数据，6 拍进行向量加载，1 拍写回寄存器，总共需要 8 拍。
>     - 第二条指令：`V2 <- V0 + V1` 需要 1 拍从寄存器读取数据，6 拍进行加法运算，1 拍写回寄存器，总共需要 8 拍。
>     - 第三条指令：`V4 <- V2 * V3` 需要 1 拍从寄存器读取数据，7 拍进行乘法运算，1 拍写回寄存器，总共需要 9 拍。
>     - 总拍数：`(8 + N - 1) + (8 + N - 1) + (9 + N - 1) = 3N + 22` 拍。
>
> * The first two instructions are parallel, and the third is serial.
>
>     **前两条指令并行执行，第三条指令串行执行**：
>
>     - 前两条指令并行执行：`max(8 + N - 1, 8 + N - 1) = 8 + N - 1` 拍。
>     - 第三条指令串行执行：`9 + N - 1` 拍。
>     - 总拍数：`(8 + N - 1) + (9 + N - 1) = 2N + 15` 拍。
>
> * Use link technology.
>
>     **使用链接技术**
>     
>     - 第一条指令：`V3 <- memory` 需要 8 拍。
>     - 第二条指令：`V2 <- V0 + V1` 需要 8 拍。
>     - 第三条指令：`V4 <- V2 * V3` 需要 9 拍。
>     - 使用链接技术，V4 的第一个结果在 17 拍后出来，随后每拍出一个结果。
>     - 总拍数：`max(8, 8) + 9 + N - 1 = N + 16` 拍。



### 5.1.3 RV64V

<div align = center> <img src="https://cdn.hobbitqia.cc/20231215204125.png" width=70%> </div>

* Loosely based on Cray-1
* 32 62-bit vector registers
    * Register file has 16 read ports and 8 write ports
* Vector functional units
    * Fully pipelined
    * Data and control hazards are detected
* Vector load-store unit
    * Fully pipelined
    * One word per clock cycle after initial latency
* Scalar registers
    * 31 general-purpose registers
    * 32 floating-point registers

> !!! Example " DAXPY (Double Precision a*X plus Y)"
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643927.png" width=80%> </div>    
>
> 

> <!-- ### Multiple Lanes: Beyond One Element per Clock Cycle 每个时钟周期可以处理多个元素。
>
> <div align = center> <img src="https://cdn.hobbitqia.cc/20231215204443.png" width=70%> </div> -->
>
> RISC-V 向量扩展 (RV64V) 中的 **`多通道（Multiple Lanes）机制`**
>
> 左侧图 (A) 展示了向量寄存器的基本组织结构。
>
> 右侧图 (B) 展示了硬件多通道的操作方式，表明多个元素组可并行处理，并简化了设计复杂度。

> ![image-20241212223225484](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412122232573.png)
>
> 处理 **`稀疏矩阵（Sparse Matrices）`** 时使用的 **`Gather-Scatter`** 方法
>
> - Gather：从指定索引位置加载数据（`vldx`）。
>
> - Scatter：将计算结果存储回指定索引位置（`vstx`）。
>
> 使用索引向量（`K` 和 `M`）**`访问非连续存储的数据`**，避免对整个矩阵进行逐元素遍历。

## 5.2 SIMD: array processor

N processing elements $PE_0$ to $PE_{N-1}$ are repeatedly set.

不同的阵列需要互相通信以连接。

> !!! Example "ILLIAC IV"    
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643647.png" width=70%> </div>
>
> 

According to the composition of the memory in the system, the **`array processor`** can be divided into two basic structures:

* Distributed memory
* Centralized shared memory

根据系统中存储器的组成，阵列处理器可分为两种基本结构：**`分布式内存和集中式共享内存`**

> **`Distributed memory 分布式内存`**

<div align = center> <img src="https://cdn.hobbitqia.cc/20231215205259.png" width=70%> </div>

- PE 代表处理器，PEN 是其对应的内存，ICN 是一个内部的互联网络。

- **`每个处理单元（PE, Processing Element）都拥有自己独立的内存（PEN, Processing Element Memory）。`**

- **`各处理单元`** 通过内部互联网络（ICN, Interconnection Network）进行通信与数据共享。

> **`Centralized shared memory 集中式共享内存`**

<div align = center> <img src="https://cdn.hobbitqia.cc/20231215205336.png" width=70%> </div>

- **PE 没有自己独占的 MM，每个 PE 都能够访问 MM**
- **`各处理单元`** 通过内部互联网络（ICN, Interconnection Network）进行通信与数据共享。
- **`内存 MM 也能通过ICN进行数据共享`**
- 网络的拓扑结构决定了处理器与内存之间的通信路径和性能。



### 5.2.1 Parallel computer design

The communication architecture of the parallel computer is the core of the system.

如果我们想让任何两个处理器都互相连接，如果都是直连，那么需要 $C_n^2 $ 个连接。

- **Definition**: A network composed of switching units according to a certain topology and control mode to realize the interconnection between multiple processors or multiple functional components within a computer system.  

  定义：由开关单元按照一定的拓扑结构和控制模式组成的网络，用于 **`实现计算机系统内多个处理器或多个功能组件之间的互联`**。

  网络中的节点代表一个处理器单元，每条边就代表一个连接。**`连接的通路越短就认为效率越高`**。

- 连接的东西包括 CPU，memory，interface，link and switch node。

  * **Interface**: It is a device that obtains information from CPU and memory and sends information to another CPU and memory. Typical devices are network interface cards.

    接口：**`它是从 CPU 和存储器获取信息，并向另一个 CPU 和存储器发送信息的设备`**。典型的设备是网络接口卡。

  * **Link**: A physical channel to transmit data bits. The link can be a cable, twisted pair or optical fiber, it can be serial or parallel, and each link has its maximum bandwidth. The link can be simplex half-duplex and full-duplex, the clock mechanism used by the link can be synchronous or asynchronous.

    链路：传输 data bit 的物理通道。

  * **Switch node**: It is the information exchange and control station of the interconnected network. It is a device with multiple input ports and multiple output ports which is able to perform data buffer storage and path selection.
  
    交换节点：它是互连网络的信息交换和控制站。它是一个具有多个输入端口和多个输出端口的设备，能够进行数据缓冲存储和路径选择。


> **!!! Note "Some key points"**
>
> * **Topology of interconnection network**
>      * **Static topology**
>
>        **静态网络：网络设定好后节点和边的连接方式就确定下来。**
>
>      * **Dynamic topology**
>
>        **动态网络：网络由很多开关组成，网络的连接方式会变化，如交叉开关我们拨动之后两个点的连接改变。**
>
> * **Timing mode of interconnection network**
>     
>     * **Synchronization system: Use a unified clock. Such as SIMD array processor**
>
>       **`同步系统：使用统一时钟`。如 SIMD 阵列处理器**
>     
>     * **Asynchronous system: No uniform clock. Each processor in the system works independently**
>     
>       **`异步系统：无统一时钟`。系统中的每个处理器独立工作**
>     
> * **Exchange method of interconnection network**
>     * **Circuit switching**
>     * **Packet switching**
>
> * **Control Strategy of interconnection network 互连网络的控制策略**
>     * **Centralized control mode: have a global controller**
>     
>       **集中控制模式：有一个 `全局控制器`**
>     
>     * **`Distributed control mode`: no global controller**
>     
>       **分布式控制模式：无全局控制器**

​            

### 5.2.2 Goal of interconnection network

* **Single-stage interconnection network**: There are only a limited number of connections at the only level to realize information transmission between any two processing units.

    单级互连网络：只有有限数量的连接，实现 **`任意两个处理单元之间的信息传输`**。

* **Multi-stage interconnection network**: It is composed of multiple single-level networks in series to realize the connection between any two processing units.

    多级互连网络：它 **`由多个单级网络串联`** 组成，实现任意两个处理单元之间的连接

N 个入端和 N 个出端会建立一个映射关系 $j \leftrightarrow f(j)$。

### 5.2.3 Single-stage interconnection network

#### 5.2.3.1 Cube

假设有 N 个入端和出端，表示为 $P_{n-1}\ldots P_1P_0$。

**`用二进制数进行表示，对应的需要` $n = log_2 N$ `个二进制位`**

这里有 n 个不同的 **`互联函数`**：（对第 i 位取反）

$$
Cube_i(P_{n-1}\ldots P_1P_0)= P_{n-1}\ldots \overline{P_i}\ldots P_1P_0
$$

> !!! Example
>
> <div align = center> <img src="https://cdn.hobbitqia.cc/20231215211043.png" width=80%> </div>
>
> <div align = center> <img src="https://cdn.hobbitqia.cc/20231215211123.png" width=80%> </div>
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643667.png" width=80%> </div>
>
> <div align = center> <img src="https://cdn.hobbitqia.cc/20231215213606.png" width=80%> </div>
>
> **3D Cube 里，任意两个点最远需要 3 步。对于 N 维的 Cube，任意两个点最远需要 $\log_2(N)$ 步。**





#### 5.2.3.2 PM2I

假设有 N 个入端和出端，表示为 $P_{n-1}\ldots P_1P_0$。

**`用二进制数进行表示，对应的需要` $n = log_2 N$ `个二进制位`**

**PM2I (Plus Minus $2^i$)** single-stage network
$$
PM2_{+i}(j)=(j+2^i)\mod N, PM2_{-i}(j)=(j-2^i)\mod N
$$

$$
0 \leq j \leq N-1, 0 \leq i \leq log_2N-1
$$

**实际上这里只有 $2\log_2(N)-1$ 个不同的函数，因为 $PM2_{+\log_2(N)}$ 与 $PM2_{-\log_2(N)}$ 是一样的。**

> !!! Example
>
> Example: N = 8
>
>  <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643985.png" width=100%> </div>
>
> **`任意两点最长的互联距离是 2。（0 可以一步到 1、2、4、6、7，再过一步可以到 3、5）`**

   

#### 5.2.3.3 Shuffle exchange network

Composed of two parts: **Shuffle** + **Exchange**

N-dimension shuffle function:

$$
shuffle(P_{n-1}\ldots P_1P_0)= P_{n-2}\ldots P_1P_0P_{n-1}
$$

> !!! Example 
>
> **`可以看到这里 000 和 111 并没有与其他点连接，因此我们需要有更多的操作`**。
>
> ![image-20241212231922740](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412122319819.png)
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643499.png" width=80%> </div>


可以看到 **`经过 3 次 shuffle 后其他点都回到了原来的位置`**，但是 000 和 111 还是没有连接。因此我们在此的基础上加上 exchange 的连线（红色是通过 $cube_0$ 实现的）。
<div align = center> <img src="https://cdn.hobbitqia.cc/20231215215546.png" width=70%> </div>   

在这里任意两个节点相连最多需要 5 步，3 exchanges + 2 shuffles.

**`从000到111，需要3次exchange，2次shuffle。`**

```
0->1 exchange
1->2 shuffle
2->3 exchange
3->6 shuffle
6->7 exchange
```

**The maximum distance of shuffle exchange network: (from the nodes numbered all “0” to all “1”)** 

**`n exchanges and n-1 shuffles, maximum distance: 2n-1`**

中间节点的距离较短，效率高，除了从全 0 到全 1 的距离远。

#### 5.2.3.4 Others

* Linear array

    开销低，但是每个点都是关键节点，一旦出现故障就会影响整个系统。
    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220123.png" width=20%> </div>   

* Circular array

    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220149.png" width=20%> </div>

    可以在点上加一些弦。
    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220232.png" width=20%> </div>

* Tree array

    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220255.png" width=40%> </div>

    可以拓展为带环的树（Tree with loop）、Binary fat tree
    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220325.png" width=60%> </div>

* Star array

    安全性较差，中间的节点非常重要。
    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220351.png" width=20%> </div>

* Grid

    在 GPU 中广泛使用。
    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220416.png" width=20%> </div>

    可以拓展为 2D torus

    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220521.png" width=20%> </div>

* Hypercube

    <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220544.png" width=60%> </div>

- cube with loop

  Cube 也可以加上环（Cube with loop）

  <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220615.png" width=20%> </div>

  <div align = center> <img src="https://cdn.hobbitqia.cc/20231215220453.png" width=70%> </div> 

  

  注意到这些都是静态网络。

### 5.2.4 Multi-stage interconnection network

![image-20241212233446231](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412122334293.png)

通过交叉开关可以实现动态的网络。（根据传过来的信号决定开关是开还是关）

开关也有多种控制方式，可以每个开关都有自己的控制器，可以有一个全局的控制，也可以分级开关，每一级是一样的。

双功能开关不能满足我们的需求，因此我们会有下面这样的交叉开关：
<div align = center> <img src="https://cdn.hobbitqia.cc/20231215222024.png" width=60%> </div>

常用的办法是级控制，每一级的开关是一样的。
<div align = center> <img src="https://cdn.hobbitqia.cc/20231215222115.png" width=70%> </div>

不同的多级网络，他们的拓扑结构、交换方式（交叉开关）、控制方式可能不一样。

> Switching unit

A switching unit with two inputs and two outputs is the basic component of various multi-level interconnection networks.

![image-20241212233552751](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412122335802.png)

![image-20241212233613581](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412122336627.png)

The statuses of switching unit:

* Straight
* Exchange
* Upper broadcast
* Lower broadcast

**`随着端口增加，这里可以有其他的开关单元，如 multicast 即组播，分组广播。`**

## 5.3 DLP in GPU

> ![image-20241223220025287](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412232200348.png)
>
> 1. **数据级并行性的利用**：SIMD 架构在处理大量数据时非常有效，例如：
>
>    - **`面向矩阵的科学计算`**：可以并行化处理大型矩阵的操作。
>    - **`面向媒体的图像和声音处理`**：如视频编码/解码、图像处理和音频处理。
>
> 2. **能效**：
>
>    - SIMD 比 **多指令多数据流（MIMD）** 架构更节能。
>
>    - SIMD 每执行一次数据操作只需获取 **一条指令**，`减少了指令获取和解码的开销`。这可以带来更快的执行时间和更低的功耗。
>
> 3. **顺序编程模型**：
>
>    - SIMD 允许程序员继续以 **`顺序思维`** 进行编程，**`尽管底层硬件是并行执行操作的`**。

* **`Heterogeneous execution model 异构执行模型`：**
  
    * CPU is the host, GPU is the device
    
    * 在这个模型中，**CPU（中央处理器）** 是主机，负责执行主要的控制流和计算任务。
    
      **GPU（图形处理单元）** 作为设备，负责执行并行计算任务，特别是在处理大量数据时表现出色。
    
* Unify all forms of GPU parallelism as **CUDA thread**

* Programming model is “**Single Instruction Multiple Thread**”

    - 这种编程模型意味着 GPU 执行的是 **`单指令多线程`** 的操作。也就是说，**`虽然有多个线程在同时运行，但它们执行的是相同的指令，只是处理的数据不同`**。
    - 这与 SIMD 的思想非常相似，即一条指令同时作用于多个数据。

**GPUs are really just multithreaded SIMD Processors**.

**GPU 本质上是多线程的 SIMD 处理器：GPU 通过 `多线程的方式来实现SIMD架构`，即在多个线程上同时执行相同的指令，但处理不同的数据。**

### Programming the GPU: CUDA

* **CUDA**: Compute Unified Device Architecture

> !!! Example
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643626.png" width=80%> </div>
>
> **`DAXPY (Double precision AX Plus Y)`** 
>
> **`where a scalar ` a ` is multiplied by a vector ` x `, and the result is added to another vector ` y `.`**
>
> - CUDA 运算：
>
>   **在内核中，每个线程对向量的特定元素 `i` 进行 DAXPY 运算。索引 `i` 由线程在程序块中的位置和程序块在网格中的位置（ `blockIdx.x * blockDim.x + threadIdx.x` ）决定。**

### Grid, Thread Blocks and Threads

* A **thread** is associated with each data element

  **`线程是GPU中最小的执行单元`**，负责处理一个具体的数据元素或计算任务。

* Threads are organized into **blocks**

  - 线程被组织成 **线程块**。**`一个线程块包含多个线程，这些线程可以并行执行相同的指令，但处理不同的数据。`**
  - 线程块内的线程可以通过 **`共享内存（Shared Memory）`** 进行通信和数据共享。

* Blocks are organized into a **grid**

  **`线程块被组织成一个网格`**。网格是 GPU 上所有线程块的集合，通常用于处理大规模的并行任务。

GPU hardware handles thread management, not applications or OS.

- **`GPU硬件` 负责线程的管理，而不是应用程序或操作系统。这意味着 `开发者不需要手动管理` 线程的创建、调度和销毁，`GPU硬件会自动处理这些任务`。**

<div align = center> <img src="https://cdn.hobbitqia.cc/20231216005125.png" width=70%> </div>

### GPU memory structures 

* **GPU memory** is shared by all *Grids* (vectorized loops).
  - **GPU memory** 是所有 **网格（Grids）** 共享的内存空间。网格可以理解为并行化的循环，GPU 内存用于 **`存储这些并行任务所需的全局数据`**。
  - GPU memory **是全局共享的**，所有线程块和线程都可以访问。
* **Local memory** is shared by all threads of SIMD instructions **`within a Thread Block`** (body of a vectorized loop). 
  - **`Local memory是线程块内的所有线程共享的内存空间`**。它用于存储线程块内的线程在执行 SIMD 指令时所需的共享数据。
* **Private memory** is private to **`a single CUDA Thread`**.
  - **私有内存** 是每个 **CUDA 线程** 私有的内存空间。每个线程都有自己的私有内存，用于存储线程的局部变量和临时数据。

<div align = center> <img src="https://cdn.hobbitqia.cc/20231216005138.png" width=80%> </div>
<div align = center> <img src="https://cdn.hobbitqia.cc/20231216005355.png" width=100%> </div>

> **NVIDIA GPU** 与 **向量机（Vector Machines）** 之间的相似性和差异性
>
> ![image-20241223221929872](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412232219929.png)
>
> **`similarity:`**
>
> 1. 两者都擅长处理 **数据级并行问题**，即在大量数据上执行相同的操作
> 2. **分散-聚集传输（Scatter-Gather Transfers）**：适合处理稀疏矩阵或非连续数据
> 3. 两者都使用 **掩码寄存器** 来控制哪些数据元素参与计算。掩码寄存器允许在并行计算中选择性地处理某些数据元素，而忽略其他元素。
> 4. 两者都具有 **大寄存器文件**，用于存储大量的临时数据。这些寄存器文件为并行计算提供了快速的本地存储，减少了访问内存的频率。
>
> **`difference:`**
>
> 1. **`NVIDIA GPU没有专门的标量处理器`**，即它不单独处理标量操作。所有的计算操作都是并行的，即使是标量操作也会在多个线程上执行。
> 2. **使用多线程隐藏内存延迟（Uses Multithreading to Hide Memory Latency）**：
>    - **`NVIDIA GPU通过多线程来隐藏内存访问的延迟。当一个线程等待内存数据时，GPU可以切换到另一个线程继续执行，从而提高整体效率。`**
> 3. **NVIDIA GPU** 拥有 **`大量功能单元`**，这些单元可以 **`并行执行多个操作`**。这种设计使得 GPU 能够在同一时间内处理大量的并行任务。

## 5.4 Loop-Level Parallelism (LLP)

**`在并行化时，循环中的操作不能有相互依赖，否则无法并行执行。`**

Focuses on determining **`whether data accesses in later iterations are dependent on data values produced in earlier iterations.`**

关键点：

- **循环级并行性** 的核心在于 **`确定后续迭代中的数据访问是否依赖于之前迭代中生成的数据值`**。
- 如果存在依赖关系，则无法并行化；如果不存在依赖关系，则可以并行化。

> !!! Example     
>
> ```c
> for (i=0; i<100; i=i+1) 
> {
>     A[i+1] = A[i] + C[i]; /* S1 */
>     B[i+1] = B[i] + A[i+1]; /* S2 */
> }
> ```
>
> - **S1** 存在 **`跨迭代的依赖`** 关系：**`A[i+1]` 依赖于 `A[i]`，这意味着每个迭代的 `S1` 必须等待前一个迭代的 `S1` 完成**，因此 **不能并行化**。
> - **S2 分析**：当前迭代中计算 `B[i+1]` 的结果依赖于 `A[i+1]` 和 `B[i]`。
>   - **`B[i]` 来源于前一迭代。`A[i+1]` 来源于当前迭代的 `S1`。**
>   - **`S2` 无法并行。**

> !!! Example 
>
> ``` c
>     for (i=0; i<100; i=i+1) {
>         A[i] = A[i] + B[i]; /* S1 */
>         B[i+1] = C[i] + D[i]; /* S2 */
>     }
> ```
>
> ​    交换 S1 S2，随后把第一次和最后一次运算提出去，可以改为下面这样，就可以并行。
>     ``` C
>     A[0] = A[0] + B[0];
>     for (i=0; i<99; i=i+1) {
>         B[i+1] = C[i] + D[i]; /* S2 */
>         A[i] = A[i] + B[i]; /* S1 */
>     }
>     B[100] = C[99] + D[99];
>     ```
>
> ![image-20241223223658691](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412232236751.png)

## 5.5 MIMD: Tread-level Parallelism

线程级的并行，称为 TLP，是由软件系统来确认的。

The threads consist of hundreds to millions of instructions that may be executed in parallel.

我们的发展从 ILP，到 TLP，再到 MIMD。

![image-20241223232712088](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412232327161.png)

* **`Multi-processor system——based on shared memory 基于共享内存`**
    - 在这种系统中，所有处理器共享一个 **`唯一的地址空间`**。这意味着所有处理器都可以访问相同的内存地址。
    - **`唯一的地址空间` 并不意味着物理上只有一个内存**。共享地址空间可以通过 **物理共享内存** 实现，也可以通过 **`分布式内存`** 加上硬件和软件的支持来实现。
    - 这种架构 **`适合需要频繁共享数据的任务`**，因为所有处理器可以直接访问共享内存，减少了数据传输的开销。
    
* **` Multi-computer system——based on message passing`**
    - 每个处理器都有自己的 **本地内存** 或 **私有内存**，**`这些内存只能被该处理器访问，其他处理器无法直接访问。`**
    - 当处理器 A 需要向处理器 B 发送数据时，A 会以 **消息** 的形式将数据发送给 B。

### 5.5.1 Shared Memory System

<div align = center> <img src="https://cdn.hobbitqia.cc/20240110201622.png" width=70%> </div>

> **`Multi-processor system——based on shared memory 基于共享内存`**

1. **多 CPU 共享相同的虚拟地址空间**：

   - 在共享内存系统中，存在多个 CPU，所有 CPU 共享相同的 **虚拟地址空间**，这些虚拟地址空间映射到 **共享的物理内存**。
   - **`Multiprocessor systems也称之为Shared Memory System`**

2. 两个处理器之间的通信非常简单：**一个处理器将数据写入内存，另一个处理器从内存中读取数据。**

3. **输入/输出设备**：

   - 多处理器系统还包括 **磁盘、网络适配器** 和其他 **输入/输出设备**。
   - If in a system, each CPU has equal access to all memory modules and input/output devices, and these CPUs are **`interchangeable`** in the operating system, then the system is a symmetric multiprocessor system **SMP (Symmetric Multi- processor)**.
   - 如果系统中的每个 CPU 对所有内存模块和输入/输出设备具有平等的访问权限，并且这些 **`CPU在操作系统中是可互换的`**，那么该系统被称为 **对称多处理器系统（SMP, Symmetric Multi-processor）**。

4. **单一操作系统**：

   - 在多处理器系统中，只有一个 **操作系统**，操作系统负责管理一系列的表。

     



### 5.5.2 Message Passing System

<div align = center> <img src="https://cdn.hobbitqia.cc/20240110202617.png" width=80%> </div>

> **` Multi-computer system——based on message passing`**

1. 在多计算机架构中，每个 CPU 都有自己的 **私有内存**，这些内存只能由该 CPU 使用，其他 CPU 无法直接访问。
2. 多计算机系统具有良好的 **可扩展性**，**`能够扩展到比Multi-processor system更大的规模`**。这是因为每个节点（node）都是独立的，可以轻松添加更多的节点来扩展系统。
3. 通信是通过 **`互连网络 ICN`** 传递 **`消息 message`** 来实现的。这种消息传递机制使得多计算机系统的编程比多处理器系统复杂得多。
4. 多计算机系统中的每个 **节点** 由一个或多个 CPU、RAM、磁盘、其他输入/输出设备以及通信处理器组成。**`每个节点都有自己的操作系统`**，至少包括操作系统的核心部分。这意味着每个节点可以独立运行，并且有自己的资源管理。



**`Communication in the system is achieved by using an interconnection network to pass messages.`**

可以分层次，ICN 也可以连接其他的 ICN，结点里可以是另一个多机系统。
<div align = center> <img src="https://cdn.hobbitqia.cc/20240110202900.png" width=80%> </div>

### 5.5.3 MIMD Architecture

* **`Different memory access models`** of MIMD **`multiprocessor system`**
    * **Uniform Memory Access (UMA)**
    * **Non Uniform Memory Access (NUMA)**
    * **Cache Only Memory Access (COMA)**

* Further division of MIMD **`multi-computer system`**
    * **Massively Parallel Processors (MPP)**
    * **Cluster of Workstations(COW)**

#### UMA

<div align = center> <img src="https://cdn.hobbitqia.cc/20240110203036.png" width=50%> </div>

**所有的物理存储器，由所有的进程一起使用，均匀共享，即 `没有进程对某个存储器有特殊的访问权限，访问的时间相同`，即不存在谁离谁更近的问题。**

进程可以有自己的拓展，比如 cache、IO、local memory。

因为他的高度共享性，UMA 也叫 **`紧耦合系统`**。

* Physical memory is uniformly shared by all processors.
* It takes the **`same time`** for all processors to access any memory word.
* **`Each processor can be equipped with private cache or private memory.`**

> **UMA** **的变形：**
>
> - **可以每个 CPU 共享一个 Memory（图一）；**
>
> - **可以每个 CPU 各自携带一个 Cache，共享一个 Memory（图二）；**
>
> - **可以每个 CPU 各自携带一个 Cache 和一个私有内存，共享一个 Memory（图三）。**
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202411301643649.png" width=80%> </div>

#### NUMA

<div align = center> <img src="https://cdn.hobbitqia.cc/20240110203501.png" width=50%> </div>

**`每个处理器能够直接访问自己的local memory，也可以通过ICN访问别的处理器的local memory，访问时间存在区别。访问的自己的快，访问别人的慢`**

NUMA 有两种拓展，

* **NC-NUMA: Non Cache NUMA**

    - **NC-NUMA** 是指没有使用缓存的 NUMA 系统。在这种系统中，远程内存访问的时间无法被隐藏，因此访问远程内存的速度较慢。
    - NC-NUMA 系统适合对内存访问速度要求不高的应用，或者在缓存利用率较低的情况下。

* **CC-NUMA: Coherent Cache NUMA**

    有自己的 cache 和目录，**`存在 cache 一致性的问题`**。当有一个数据改了，如何保证其他 cache 里的数据的正确性。

<div align = center> <img src="https://cdn.hobbitqia.cc/20240110204714.png" width=55%> </div>

**`NUMA系统的特点`**

* All CPUs share an uniform address space

  在 NUMA 系统中，所有 CPU 共享一个 **统一的地址空间**。这意味着所有 CPU 都可以访问相同的内存地址，**`但访问速度可能不同。`**

* Use LOAD and STORE instructions to access remote memory

  CPU 可以通过执行 **`LOAD和STORE指令`** 来访问远程内存。**`远程内存`** 指的是不属于该 CPU 本地内存的内存区域。

* Access to remote memory is slower than access to local memory

  访问 **远程内存** 的速度比访问 **本地内存** 要慢。这是因为远程内存需要 **`通过互连网络`** 进行访问，而本地内存则可以直接访问。

* The processor in the NUMA system can **`use cache`**

> !!! Note "UMA and NUMA"
>
> * UMA is also called **`symmetric (shared-memory) multiprocessors (SMP)`** or **centralized shared-memory multiprocessors**.
> * NUMA is called **`distributed shared-memory multiprocessor (DSP)`**.
>
> <div align = center> <img src="https://cdn.hobbitqia.cc/20240110205135.png" width=100%> </div>
>
> 可以看到 UMA 有 shared cache，因此一致性是保证的。

#### COMA

<div align = center> <img src="https://cdn.hobbitqia.cc/20240110205326.png" width=50%> </div>



<div align = center> <img src="https://cdn.hobbitqia.cc/20240110205525.png" width=80%> </div>

* COMA is a special case of NUMA. There is no storage hierarchy in each processor node, and all caches form a uniform address space.

  COMA 是 NUMA 的一种特例。每个处理器节点中没有存储层次结构，**`所有缓存构成一个统一的地址空间。`**

* Use the distributed cache directory for remote cache access. When using COMA, the data can be allocated arbitrarily at the beginning, because it will eventually be moved to the place where it is used at runtime.

  使用分布式缓存目录进行远程缓存访问。使用 COMA 时，**`数据可以在开始时任意分配，因为它最终会被移动到运行时使用的地方。`**

### 5.5.4 Cache Coherence

> ![image-20241224000734709](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240007765.png)
>
> **`区分memory consistency和Cache Coherence`**
>
> - **`Memory consistency`**
>
>   **内存的一贯性，保持先写后读的顺序**
>
> - **`Cache Coherence`**
>
>   **多个 cache，一个 cache 的内容发生修改，其他 cache 相同位置失效**





 

> ![image-20241225232847533](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412252328607.png)
>
> - write through：data is written to memory
>
> - Write around (no write allocate)  
>
>   **常与 write through 搭配**
>
>     * The block is only written to main memory
>     * It is not stored in the cache.
>
> **`hit采用write through，同时写入到内存和cache，miss 采用write around，仅仅写回内存`**

> ![image-20241225232934728](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412252329776.png)
>
> - Write back：写回到 cache
>
> - Write allocate  
>
>   The block is loaded into the cache on a miss before anything else occurs.  
>
>   **`读取到cache的时候，如果发生替换，还需要检查被替换的数据dirty bit是否为1，是否需要写回内存`**
>
>   **常与 write back 搭配**
>
> **`hit采用write back，只写cache，miss 采用write allocation，将数据读到cache，写cache`**

![img](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412252342160.png)

**Coherency：指的是每一个 CPU 从 Cache 中读数据的时候，一定要读到最新写过的数据。**

**`每一个CPU都带有自己的Cache，因此同一个内存数据块可能被拷贝到多个Cache中，因此可能导致Cache Coherency问题。`**

为了解决这个问题，使用一定的协议，例如：**`对于UMA结构，常常采用Bus Snooping协议；对于NUMA结构，常常采用Directory Based协议。`**

① Bus Snooping protocol（总线窥探协议）

- 在窥探协议中，所有处理器都会 **窥探总线**。当一个处理器修改了私有缓存中的数据时，它会 **`通过总线广播无效信息或更新后的数据`**，以使其他处理器的缓存中的副本无效或更新。

② Directory Based protocol（基于目录的协议）

- 目录协议使用 **`一个目录来记录系统中哪些处理器在缓存中拥有某些存储块的副本`**。
- 当一个处理器想要写入一个共享块时，它会通过目录以 **`点对点的方式向拥有该块副本的处理器发送无效信号`**，从而使所有其他副本无效。
- **`减少了广播带来的通信开销，适合远程内存访问较慢的情况。`**

![image-20241224001724899](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240017948.png)

**`write through与write back的区别：write through能够保持内存中的数据总是最新的，write back只写回cache，cache中存在脏数据`**

#### MSI

![image-20241224002215182](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240022243.png)

**`read miss直接从内存中读取数据，read hit从cache读取数据`**

**`write miss：修改内存中数据。write hit修改cache和内存中数据，同时其他cache item失效`**



<center>
    <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006725.png" alt="img" />
</center>

**每一个 Cache 块的状态分为三种：`invalid，shared，modified。shared表示该块虽然位于private cache，但是是共享的；modified表示该块在private cache中被修改，意味着是独占的`**



<center>
    <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412252349031.png" alt="image-20241225234945991" />
</center>



<center>
<img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412252350250.png" alt="image-20241225235031214" />
</center>





#### MESI

**状态扩展：扩展出四种状态（MESI 协议）：**

**![手机屏幕截图  描述已自动生成](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006733.png)**

 独占：表示缓存块只驻留在单个缓存中，但是是干净的



**![img](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006710.png)**

- **Invalid** **状态：表明本 CPU 的 Cache 中的对应 `数据块是无效` 的（可能 `被其他 CPU 更改过`）。**

- **Shared** **状态：表明对应数据块在多个 CPU 的 Cache 中，同时这些数据块的内容和内存中保持一致，是最新内容（这些数据块是内存中同一块的拷贝）。因此，各个 CPU 可以共同读 Cache 中的这块数据。**

- **Exclusive** **状态：处于 Exclusive 的数据块只有在本 CPU 对应的 Cache 中才有缓存，并且没有被修改。因此，对应在内存中的数据块是最新的。**

- **Modified** **状态：处于 Modified 的数据块只有在本 CPU 对应的 Cache 中才有缓存，并且产生了一定的修改。同时，`对应修改还没有更新到内存中去`。**

 

![image-20241225235301647](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412252353694.png) 

**上图中，Write Snooping Hit 表示，当前 CPU 通过总线【窥探到】其他 CPU 对应的 Cache 对当前数据块的写命中了。Read Snooping Hit 表示，当前 CPU 通过总线【窥探到】其他 CPU 对应的 Cache 对当前数据块读命中了。**

> **需要注意：处于任何状态下，只要 Write Snooping Hit，则表示这一块被其他的 CPU 更改了。因此在本 CPU 的 Cache 中，对应块就应该设置成 Invalid。**

**·例 1：**

![image-20241225235636964](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412252356001.png)

**假设有两个 CPU A 与 B，初始状态时，每一个 CPU 对应的 Cache 中每一个块都处于 Invalid 状态。同时，`采取 Write Back 与 Write Allocate 策略`。**

- **`invalid 到 modified`**：**假如 CPU A 对于某一个地址发起一个写操作，该地址没有出现在 A 与 B 的 Cache 中，那么就需要从内存中加载对应地址的块。将对应地址的块加载到 CPU A 中后，CPU A 对该块进行更改。然后，CPU A 中该块的状态就会变成 Modified，表明与内存数据不同步。**

- **`modified 到 shared`**：**然后，CPU B 对于该地址发起一个读请求。此时，需要 `进行【远程读】`。读取了 CPU A 刚刚更改过的数据，`因此 CPU B 的 Cache 中也保存了该块的最新副本`。同时，CPU A 与 CPU B 对应的 Cache 中对应块的状态都将会变为 `Shared`。**

  > **【注意】CPU A 利用总线，收到 CPU B 的远程读请求后，`会首先将 CPU A 刚刚更改掉的数据块【写回内存中】，再将这块数据给 CPU B 去读取`。此时，相同的最新数据块处于两个 CPU 的 Cache 中，并且与内存中保持一致。给 CPU B 读取后，两个 CPU 的 Cache 中对应块都会变成 Shared 状态。**

- **`shared 到 modified，shared 到 invalid`**： **此时，CPU A 又对刚刚那个地址 `发起了写操作`，因此在 CPU A 的 Cache 中，对应的块的状态就会 `变回 Modified，表示与内存数据不同步`。与此同时，CPU A 的 Cache 中被更改的数据块也与 CPU B 的 Cache 中的对应块不同步。因此，此时 `CPU B 的 Cache 中的对应块的状态由 Shared 变成 Invalid`，表明对应块已经被其他 CPU 写过了。【【【FLAG】】】**

  > **【注意】假设此时 CPU A 中对应块的状态为 Modified，CPU B 中对应块状态为 Invalid，这对应前面叙述的【【【FLAG】】】时刻。此时，CPU B 对该地址发起了写操作，该写操作请求会通过总线进行传播。传播到 CPU A 处，首先，CPU A 会阻止这一请求，因为 CPU A 的 Cache 中对应数据块是 Modified，`因此【先将 CPU A 刚刚更改过的数据（即标有 Modified 的对应块）写回内存中】，然后再将对应块的状态更改为 Invalid`。此时，CPU B 发现之前发送的写操作请求没有得到回应，会【再次发送给一个写操作请求】。此时，CPU B 再次发送写操作请求的时候，就会发现 `此时所有 CPU 的 Cache 中对应块都是 Invalid`，即在 Cache 中已经不存在该副本了（在此之前 CPU B 的对应块已经变成了 Invalid）。因此，`CPU B 发生了写 Miss，就会从 Memory 中加载数据`。【注意，这里加载的数据块就是刚刚 CPU A 写回的那个数据块】。加载完毕后，CPU B 在自身的 Cache 中对该数据块进行修改，同时将该数据块的状态改为 Modified。**
  >
  > **当然，如果当一个 CPU 需要写一个数据块，而该数据块在自身 Cache 中是 Invalid，但是在 `其他 Cache 中是 Exclusive 或 Shared 状态时`，当需要进行写操作的 CPU 第一次发送写请求的时候，对应其他 Cache 就 `无需` 将对应块的数据先写回内存中（`因为 Exclusive 状态与 Shared 状态的块已经和内存中保持一致`），这些块只需要简单地将状态置为 Invalid 即可。**
  >
  > **假如当一个 CPU 需要写一个数据块，且该数据块在自身 Cache 中是 Valid 状态，那么就直接更改自身 Cache 中的对应块，并且将其他 CPU 的 Cache 的对应块全部改为 Invalid 即可。因为此时在自身的 Cache 中写命中，无需再从内存中加载。**

- **`invalid 到 modified， modified 到 invalid`**： **然后，CPU B 又对刚刚那个地址发起了写操作，因此在 CPU B 的 Cache 中，对应的块的状态就会由 Invalid 变回 Modified，表示与内存数据不同步。同时，CPU B 的 Cache 中被更改的数据块也与 CPU A 的 Cache 中的对应块不同步。因此，此时 CPU A 的 Cache 中的对应块的状态由 Modified 变成 Invalid，表明对应块已经被其他 CPU 写过了。**



> **`对于读请求，只要对应块的状态不是 Invalid，都是可以进行读操作的`。**
>
> - **某个 CPU 进行远程读的时候，如果被读取的 Cache 块的状态是 `Modified`，那么就需要 `先将这一块写入内存中，然后再共享读`（即将读与被读的 CPU 的 Cache 的对应块都设置为 Shared）。**
> - **如果被读取的 Cache 块的状态是 `Exclusive`，那么就 `无需` 先将这一块写入内存中，`因为这一块未被修改过`，只需将读与被读的 CPU 的 Cache 的对应块都设置为 Shared 状态即可。**
>
> **`也就是说，读操作是可以远程读的，都可以看作读命中；但写操作不能远程写其他 CPU 的 Cache，只有当自身的 Cache 中对应块不是 Invalid 的时候，才能看作写命中，否则都需要从内存中加载（从内存中加载然后再更改可以看作是【先读再改】）`。从内存中加载，就需要让对应块为 Modified 状态（如果有）的其他 Cache 中的对应 Cache 块先写回内存，切换状态为 Invalid；并且让其他所有对应块状态为 Exclusive 或 Shared 的 Cache 中的对应块状态切换为 Invalid。**



 

 

 

 

 

 

 

**·例 2：**

**![img](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006235.png)**

- **① CPU1 读块 A**

  此时，由于仅仅是读操作，因此读出来的数据块和内存中保持一致，因此 **`该块的状态为 Exclusive`**，表示目前只有这一个 CPU 的 Cache 中拥有块 A，并且块 A 的内容与内存中一致。

- **② CPU2 读块 A**

  此时，由于 CPU1 的 Cache 中的对应块状态为 Exclusive，因此不需要先将 CPU1 的 Cache 中的对应块写回内存中。此时，**`CPU1 与 CPU2 的 Cache 中对应块状态都变为 Shared`**。

- **③ CPU2 写块 A**

  写命中。**`将 CPU1 的 Cache 中块 A 的状态改为 Invalid`**（这里不用使得 CPU1 的 Cache 中块 A 先写回内存，因为处于 Shared 状态），然后直接在 CPU2 的 Cache 中更改块 A 即可，状态变为 Modified。

- **④ CPU3 读块 A**

  此时，由于 CPU2 的 Cache 中的对应块状态为 Modified，因此 **需要先将 CPU2 的 Cache 中的对应块写回内存中**。然后，CPU2 与 CPU3 的 Cache 中对应块状态都变为 Shared。

- **⑤ CPU2 写块 A**

  写命中。**`将 CPU3 的 Cache 中块 A 的状态改为 Invalid`**（这里不用使得 CPU3 的 Cache 中块 A 先写回内存，因为处于 Shared 状态），**`然后直接在 CPU2 的 Cache 中更改块 A 即可，状态变为 Modified。`**

- **⑥ CPU1 写块 A**

  写缺失。此时 CPU1 中块 A 的状态为 Invalid，同时 CPU2 中块 A 的状态为 Modified。**`因此需要先将 CPU2 的 Cache 中块 A 先写回内存`**，因为处于 Modified 状态。然后，将 CPU2 的 Cache 中块 A 的状态改为 Invalid。然后需要 **【先读再写】**，**`首先从内存中把刚刚 CPU2 写回内存的块 A 加载到 CPU1 的 Cache 中，然后再在 CPU1 的 Cache 中更改块 A，同时状态变为 Modified`。**

 

**② Directory Based 协议**

<center>
    <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006290.png" alt="img" />
</center>

shared: 一个或多个节点缓存了块，内存中的值是最新的

modified: 正好有一个节点拥有缓存块的副本，内存中的值已过时

<center>
<img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412260012437.png" alt="image-20241226001226392" />
</center>

**三种状态与 Bus Snooping 协议中类似，读写操作的状态转化也类似。**

 

 

 

 

**![img](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006497.png)**

**![img](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006570.png)**

 

 

 

 

 

 

 

 

 

**·内存一致性（Consistency）**

  **例如：**

**![图形用户界面, 应用程序, Word  描述已自动生成](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006607.png)**

需要 **严格** 保持各个处理器进行的操作 **在顺序上** 的一致性，以保证结果的正确性。

 **`每个处理器上的访问都按顺序进行,不同处理器上的访问任意交错进行`**

**Coherent** **：保证 Cache 的数据块和 Memory 中一致。**

**Consistency** **：保证操作之间前后顺序一致。**

 

 

 

 

 

 

**·宽松一致性（Relaxed Consistency）**

  **核心思想：** 允许读写操作乱序完成，但用一些**`同步的信号`**来保持某些操作的顺序。

**有些原则需要保持，有些原则可以适当的放松：**

**![img](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412240006762.png)**

**基于严格顺序的操作执行，某些条件可适当放松。**

![](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412260015828.png)

![image-20241226001613164](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202412260016205.png)