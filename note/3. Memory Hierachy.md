

# 三. Memory Hierarchy

[TOC]



## 3.1 Introduction

<div align = center> <img src="https://cdn.hobbitqia.cc/20231028111607.png" width=80%> </div>

  <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410241149995.png" width=80%>
## 3.2 Four Questions for Cache Designers

**Caching** is a general concept used in processors, operating systems, file systems, and applications.

* **Q1**: Where can a block be *placed* in the upper level/main memory? (**Block placement**)
    * Fully Associative, Set Associative, Direct Mapped
* **Q2**: How is a block *found* if it is in the upper level/main memory? (**Block identification**)
    * Tag/Block
* **Q3**: Which block should be *replaced* on a Cache/main memory miss? (**Block replacement**)
    * Random, LRU, FIFO
* **Q4**: What happens on a *write*? (**Write strategy**)
    * Write Back or Write Through (with Write Buffer)

### Q1: Block Placement

* Direct mapped  

    **`Block can only go in one place in the cache`**  

    块只能去一个地方，通常取模

* **Fully associative 全相联**  

    **`Block can go anywhere in cache.`**  

    哪里空了去哪里

* **Set associative 组关联**  

    **`Block can go in one of a set of places in the cache.`**  

    **A set is a group of blocks in the cache.**  

    > **set associative 是上面两种方法的集合。先用 block address mod number of sets in the cache 计算出当前 block 应当位于哪个 set，然后在对应的 set 空闲的任意 block 进入插入**

    **如果一个 set 有 n 个 block，那么就将该 cache 称之为 n-way set associative**

    **Direct mapped 相当于 1-way set associative, Fully associative 相当于 m-way set-associative(for a cache with m blocks)**

<center>
        <div align=center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202405171500823.png" width = 80%/> </div>
</center>




注意这里应该是对 Set0 里面的 tag 比较。

**先根据 index 找到对应的 set，然后 tag 跟 set 里面的每一个 entry 的 tag 进行比较**

![image-20240522104112149](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410272119102.png)

### Q2: Block Identification

* Tag  

  - Every block has an address tag that stores the main memory address of the data stored in the block.

    每个块都有一个地址标签，用于存储块中存储的数据的主内存地址。

  - When checking the cache, the processor will compare the requested memory address to the cache tag -- if the two are equal, then there is a cache hit and the data is present in the cache

    在检查缓存时，处理器会将请求的内存地址与缓存标签进行比较——如果两者相等，则存在缓存命中，并且数据存在于缓存中

  直接映射只需要比较一个块的 tag, 级相联需要比较 set 里所有块的 tag.  

* Valid bit

The Format of the Physical Address

* The Index field selects  

  - 全相联没有 index. 

    * The set, in case of a set-associative cache 组关联 $\log_2(sets)$
    * The block, in case of a direct-mapped cache 直接映射 $\log_2(blocks)$

* The Byte Offset field selects  

  由一个块内 byte 的数目决定. $\log_2(size \ of \ block)$

* The Tag is used to find the matching block within a set or in the cache

<div align=center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202405171510138.png" width = 80%/> </div>

> **Example "Direct-mapped Cache Example (1-word Blocks)"**

<center>
        <div align=center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202405171511765.png" width = 80%/> </div>
</center>

**一个 block 有 4 个 byte，对应 byte offset 的位数为 2.**

**cache 一共有 4 个 cache entry，对应的 index 的位数为 2**

**从 cache 中取 0x04 的内容时，比较对应 index 的 entry 的 tag 是否相等，比较 valid bt 是否为 1**

> **Example "Fully-Associative Cache example (1-word Blocks)"**

  <center>
        <div align=center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202405171512242.png" width = 80%/> </div>
  </center>


**这种方式的好处是可以降低 miss rate，坏处是每次需要跟所有 block 比较是否 hit：**

**由于没有 index，内存地址的 tag 需要与 cache block address 中所有的 tag 进行，如果相等再去检查 valid bit 是否为 1.对于较大的 cache，耗时太长**

> **Example "2-Way Set-Associative Cache"**

前提：cache 一共 4 个 block，一个 block1 个 word

分成两组的话，一个 set 有两个 block

<center>
        <div align=center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202405171512982.png" width = 80%/> </div>
</center>

### Q3: Block Replacement

* **Random** replacement - randomly pick any block
* **Least-Recently Used (LRU)** - pick the block in the set which was least recently accessed

    需要额外的位数来记录访问的时间。一般我们用的是近似的 LRU。

* **First In, First Out (FIFO)** - Choose a block from the set which was first came into the cache

> !!! Example "Strategy of Block Replacement"
>
> Suppose: 
>
> * Cache block size is 3, and access sequence is shown as follows.  
>   
>     2, 3, 2, 1, 5, 2, 4, 5, 3, 4
>
> * FIFO, LRU and OPT are used to simulate the use and replacement of cache block. （OPT 是一种理想情况，用来衡量算法性能）
>
>     * FIFO
>
>         <div align = center> <img src="https://cdn.hobbitqia.cc/20231028141303.png" width=80%> </div>
>
>     * LRU
>
>         <div align = center> <img src="https://cdn.hobbitqia.cc/20231028141323.png" width=80%> </div>
>     
>     * OPT: **`知道未来会有怎么样的输入`**
>
>         <div align = center> <img src="https://cdn.hobbitqia.cc/20231028141338.png" width=80%> </div>

Hit rate is related to the replacement algorithm, the access sequence, the cache block size.

<img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410272138674.png" alt="image-20241027213824603" width=80%/>

cache 中 block 的数量也会影响 cache 的命中率

<img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410272138765.png" alt="image-20241027213855704" width=80%/>



#### Stack replacement algorithm

有些算法随着 N 增大命中率非下降，有些算法随着 N 增大命中率反而会下降。  

我们把随着 **`N 增大命中率非下降`** 的算法称为 stack replacement algorithm。

$B_t(n)$ represents the set of access sequences contained in a cache block of size $n$ at time $t$.

* $B_t(n)$ is the subset of $B_t(n+1)$.

**`LRU replacement algorithm is a stack replacement algorithm, while FIFO is not.`**  

> 对于 LRU 策略这样的栈替换策略，其 Cache 的大小越大，命中率增加（至少不会降低）。
>
> 对于 FIFO 策略这样的非栈替换策略，其 Cache 的大小增加，其命中率反而可能减少。
>
> ![表格  描述已自动生成](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410272343576.png)
>
> ![电脑屏幕的照片  中度可信度描述已自动生成](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410272343583.png)
>
> **上图中，** $B_7(n)$ **不是** $B_7(n+1)$ **的子集。**



> !!! Example "Using LRU"
>
> **`For LRU algorithm, the hit ratio always increases with the increase of cache block.`**
>
> 用栈来模拟 LRU，栈顶是最近访问的，栈底是最久未访问的，每次要替换的时候，替换栈底的元素。通过下面的图可以快速看到栈大小为 n 时的命中率。
>
> - **先开一个大小为 6（充分大）的栈。**
>
>    <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410241149896.png" width=60%> </div>
>
>   按照访问的顺序逐个进行压栈，如果某一个已经在栈中的元素再次被访问到，那么就将该元素提升到栈顶位置。
>
> - **$S_t(1)$ 就表示栈的最大容量为 1 的情形，没有产生命中。$S_t(2)$ 就表示栈的最大容量为 2 的情形，产生了一次命中，依此类推。$S_t(5)$ 就表示栈的最大容量为 5 的情形，产生了五次命中。**
>
>   **`并且符合stack replacement algorithm的性质，$B_t(n)$ is the subset of $B_t(n+1)$，因为随着n的增大，hit得到继承`**

#### LRU Implementation - Comparison Pair Method

> **LRU** **替换策略的硬件方式实现**

如何只通过门和触发器来实现 LRU 算法？—— **Comparison Pair Method**

* Basic idea

    Let each cache block be combined in pairs, use a comparison pair flip-flop to record the order in which the two cache blocks have been accessed in the comparison pair, and then use a gate circuit to combine the state of each comparison pair flip-flop, you can find the block to be replaced according to the LRU algorithm.

    **`让任何两个 cache 块之间两两结对，用一个触发器的状态来代表这两个块的先后访问顺序（比如 1 表示 A 刚被访问，0 表示 B 刚被访问）。`** 通过门电路对触发器的状态进行逻辑组合，找到最久未被访问的块。

>  !!! Example "Comparison Pair Method"
>
> **这里有 3 个 cache blocks A, B, C。那么我们需要 3 个触发器来记录之间的状态。假设 $T_{AB}=1$ 表示 块 A 先于块 B 被访问，$T_{AC}, T_{BC}$ 同理。**
>
> <div align = center> <img src="https://cdn.hobbitqia.cc/20231028143355.png" width=80%> </div>
>
> 其中，$A_{LRU}$，表明下一次应当被替换的块是块 A，块 B、块 C 同理。
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410241149038.png" width=80%> </div>
>
> **同时，在每一个块被访问过后，都应当更新 $T_{AB},T_{AC},T_{BC}$ 信号的值：**



* **Hardware usage analysis**

    假设有 p 个 cache blocks, 我们需要 $C_p^2=p\cdot (p-1)/2$ 个触发器。  
    
    当 $p$ 超过 8 时，需要的触发器过多，这个算法就不适用了。
    
    ![image-20241027235546595](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410272355623.png)

### Q4: Write Strategy

> **write hit:**
>
> - **write back**
> - **write through**

* **If the data is written to memory, the cache is called a write through cache**

  * Can always discard cached data - most up-to-date data is in memory  
  * Cache control bit: only a *valid* bit
  * memory (or other processors) always have latest data

  **`好处是：cache和memory具有一致性，cache内的block需要被替换时，直接丢掉即可。同时只需要一个valid bit，不需要dirty bit检查是否修改过`**

* If the data is NOT written to memory, the cache is called a **write-back** cache

  * Can’t just discard cached data - may have to write it back to memory
  * Cache control bits: both *valid* and *dirty* bits
  * much lower bandwidth, since data often overwritten multiple times

  **`坏处是：需要一个dirty bit，需要检查是否被修改过。cache和memory不一致。`**

  **`好处是：对某一个数据执行多次的写入修改操作时，只需在最后写入到内存中，速度快`**

写回需要时间，我们需要 write stall.  

> **`Write stall -- When the CPU must wait for writes to complete during write through.`**  

我们往往使用 write buffer.  

* A small cache that can hold a few values waiting to go to main memory.

* **It does not entirely eliminate stalls since it is possible for the buffer to fill if the burst is larger than the buffer.**  

  **`buffer 可能被填满，不能完全避免 write stall`.** 

<div align=center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202405171603547.png" width = 80%/> </div>

**`存在问题：当我如果要读的时候，数据还在 buffer 里没有被写入内存。因此我们需要先在 buffer 里面比较，如果没有再在内存里找。`**

> **Write miss**
>
> - **write allocate**
> - **write around(no write allocate)**

* Write allocate  

  The block is loaded into the cache on a miss before anything else occurs.  

  **`读取到cache的时候，如果发生替换，还需要检查被替换的数据dirty bit是否为1，是否需要写回内存`**

  **常与 write back 搭配**

* Write around (no write allocate)  

  **常与 write through 搭配**

    * The block is only written to main memory
    * It is not stored in the cache.

> 区分 write Through， write back， write allocation 操作
>
> ![image-20240515110208656](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410272358587.png)
>
> 如果执行的是 no-write allocate
>
> - ·`由于初始状态cache是空的，所以Write miss，此时执行的是no-write allocate（也就是write around），直接将Mem[100]写入到内存中，cache上不保存Mem[100]`
> - `再次执行write Mem[100], write miss, 依然直接写入到内存，不保存在cache中`
> - `Read Mem[200], Read miss，此时需要从内存中取出Mem[200],并将Mem[200]保存到cache中，然后读取Mem[200]`
> - `write Mem[200], write hit`
> - `write Mem[100], write miss`
>
> 如果执行的是 write allocate
>
> - `由于初始状态cache是空的，所以Write miss，此时执行的是write allocate，需要先把对应的block拿到cache，然后再写入数据`
> - `再次执行write Mem[100], write hit, 因为cache中已经存在Mem[100]`
> - `Read Mem[200], Read miss，此时需要从内存中取出Mem[200],并将Mem[200]保存到cache中，然后读取Mem[200]`
> - `write Mem[200], write hit`
> - `再次执行write Mem[100], write hit, 因为cache中已经存在Mem[100]`

## 3.3 Memory System Performance

![image-20241028000253935](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280002963.png)

<div align = center> <img src="https://cdn.hobbitqia.cc/20231028144742.png" width=80%> </div>
<div align = center> <img src="https://cdn.hobbitqia.cc/20231028145133.png" width=80%> </div>

**AMAT 的计算：用总的内存访问时间（命中时间+缺失率 *缺失代价）（以时钟周期为单位）除以所有需要访问内存的次数（包含了 ld/st 指令与取指两方面）。**

**AMAT 的单位是时钟周期。**

> ![image-20241028000620978](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280006011.png)
>
> ![image-20241028000720324](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280007355.png)
>
> ![image-20241028000932006](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280009038.png)
>
> ![image-20241028000941896](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280009935.png)
>
> 
>
> ![image-20241028001009034](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280010070.png)
>
> ![image-20241028001016400](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280010347.png)
>
> ![image-20241028001030616](https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410280010894.png)

How to improve

* Reduce the miss penalty
* Reduce the miss rate
* Reduce the time to hit in the cache
* Reduce the miss penalty and miss rate via parallelism

## 3.4 Virtual Memory

<center>
    <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410311012975.png" alt="image-20241031101222876" width=80%/>
</center>



物理内存有限，虚拟内存让用户体验到一个抽象的更大的内存。

* Why virtual memory?

    可以让进程使用不连续的物理内存空间（虚拟地址上是连续的）；更好地隔离不同进程。

    > prior Virtual Memory
    >
    > **`采用contiguous allocation，连续分配，direct memory access直接内存访问`**
    >
    > <center>
    >     <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410311022429.png" alt="image-20241031102200361" width = 80%/>
    > </center>
    >
    > **`使用virtual memory之后，能够实现memory protect and process isolation`**
    >
    > <center>
    >     <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410311023917.png" alt="image-20241031102316825" />
    > </center>
    >
    > - Easier memory management
    > - share a smaller amount of physical memory among many processes
    > - physical memory allocations need not be contiguous
    > - memory protection， process isolation
    > - introduces another level of secondary storage

* virtual-physical address translation

* memory protection/sharing among multi-program





Virtual Memory = Main Memory + Secondary Storage

* Virtual Memory Allocation
    * Paged virtual memory

        **`page: fixed-size block`**
    
        page number + offset 寻址

    - Segmented virtual memory
    
      **`segment: variable-size block`**
    
    <div align = center> <img src="https://cdn.hobbitqia.cc/20231104161447.png" width=80%> </div>

> !!! Info "Paging vs Segmentation"
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410241149670.png" width=90%> </div>
>
> 分页式的易于实现，方便替换。现在常用段页式结合，或者纯页式。



### How virtual memory works?

Cache 的四个问题在虚拟内存中都有对应。

* Q1. Where can a block be placed in main memory?

    采用全相联的方式可以有效降低缺失代价。这种方法允许任何一个内存块被放置在主存的任何位置，从而提高了缓存的灵活性并降低了缺失率。

* Q2. How is a block found if it is in main memory?

    虚拟地址由页号和偏移量组成。页号用于索引页表，页表存储了虚拟页与物理页的映射关系。通过查找页表，可以找到对应的物理页号，从而访问主存中的数据。
    <div align = center> <img src="https://cdn.hobbitqia.cc/20231104161916.png" width=80%> </div>、
    
    1. 通过虚页号来查找页表
    2. 找到对应项中的 PPN（物理页号）
    3. 和页内偏移量合并，组成物理地址。
    
* Q3. Which block should be replaced on a virtual memory miss?

    Least Recently Used (LRU) block, with use/reference bit.

    **利用虚页号查找页表得到物理页号，但是该页有两种情况：此时就放在内存中，或者此时不在内存中。**

    **因此，如果在内存中缺失，就需要从磁盘中将对应页拿到内存中。此时产生页的替换，我们使用 LRU 策略。**

* Q4. What happens on a write?

    采用写回（Write-back）策略。**`写入到cache，但是不写入到内存，开销太大`** 只有在块被替换时才会将修改写入主存。每个缓存块通常还会有一个脏位（dirty bit），用来标记该块自从加载后是否被修改过。如果脏位被设置，则在替换该块时必须将其内容写回主存

### Page Table

<center>
    <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410311042913.png" alt="image-20241031104237808" width=80%/>
</center>

* Page tables are often large

    ***e.g.*** 32-bit virtual address, 4KB pages, 4 bytes per page table entry.  
    
    page table size: $(2^{32}/2^{12}) \times 2^2 = 2^{22}$ bytes = $4$ MB
    
* Page tables are stored in main memory

    页表通常存储在主存中
    
* Logically two memory accesses for data access:
    * one to obtain the physical address from page table;
    
      **第一次访问**：访问页表，以从虚拟地址中获取对应的物理地址。
    
    * one to get the data from the physical address;
    
      **第二次访问**：使用获得的物理地址访问实际数据。

正常来说页表需要两次内存访问，访问效率低下，因此我们需要 cache page table，即 TLB。

**Translation lookaside buffer (TLB)**

* tag: portions of the virtual address (VPN);
* data: a physical page frame number (PPN), protection field, valid bit, use bit, dirty bit;
  - 保护字段（Protection Field）：指示该页的访问权限（例如，读、写、执行权限）。
  - 有效位（Valid Bit）：指示条目是否有效，如果无效，则 TLB 条目不能被用来进行地址转换。

> !!! Example
>
> 1. **查找 TLB**：使用提取的 **`VPN`** 作为标签 **`tag`** 去匹配 TLB 中的条目。
> 2. **匹配成功**：
>    - 如果找到匹配的条目，TLB 将返回相应的 PPN，接着将偏移量 page offset 与 PPN 结合，形成完整的物理地址。
>    - 这一过程只需一次内存访问，显著提高了访问效率。
> 3. **匹配失败**：
>
>      - 如果 TLB 未命中（即没有找到对应的条目），系统需要访问页表，这时将执行两次内存访问：第一次获取物理地址，第二次获取数据。
>
>       - 一旦找到物理地址，系统将新条目加载到 TLB 中，以便后续访问更快。
>
><center>
>  <div align = center> <img src="https://cdn.hobbitqia.cc/20231104170717.png" width=80%> </div>
><div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410241149404.png" width=80%> </div>
>  <div align = center> <img src="https://cdn.hobbitqia.cc/20231104171003.png" width=80%> </div>
> </center>
> 
> 

### Page Size Selection

* Pros of *larger* page size
    * Smaller page table, less memory (or other resources used for the memory map);

        较大的页面意味着系统可以管理的页数减少，从而使页表的大小减小。这减少了系统在管理页表时所需的内存资源。
    
    * Larger cache with fast cache hit;

        页更大，所以 cache 命中的时间更短（因为我们需要遍历的页更少）。

    * Transferring larger pages to or from secondary storage is more efficient than transferring smaller pages;

        从辅助存储（如硬盘）传输数据时，较大的页面可以一次传输更多数据，减少了传输次数。所以更高效，小页可能需要搬运多次。

    * Map more memory, reduce the number of TLB misses;

        较大的页面能够映射更多的内存，因此TLB中记录的条目数量可能会增加，降低了TLB未命中的概率。

* Pros of *smaller* page size
    * Conserve storage
      
        When a contiguous region of virtual memory is not equal in size to a multiple of the page size, a small page size results in less wasted storage.

        当虚拟内存中的某个连续区域的大小不是页面大小的整数倍时，较小的页面可以减少内部碎片，即未使用的空间。
    
    

Use both: **multiple page sizes**

> !!! Note "Address Translation"
>
> **虚拟内存和多级Cache结合**
>
> <div align = center> <img src="https://zn-typora-image.oss-cn-hangzhou.aliyuncs.com/typora_image/202410241149464.png" width=100%> </div>
>
> 1. **对于一级Cache，它的大小设定与一个Page的大小相同。**
>
>    **在一级Cache中查找的时候，利用的是虚拟地址中的页内偏移量。**
>
>    **将虚拟地址中的页内偏移量分为Cache Index部分和Block Offset部分。**
>
>    **利用Cache Index在Cache中找到对应Entry，并利用TLB中得到的`物理页号 PPN `作为Tag在Cache中进行比对。**
>
> 2. **如果L1缓存未命中，则系统会利用`完整的物理地址`在L2缓存中查找。**
>
>    **物理地址被拆分为 `Tag (19 bits)`、`L2 Cache Index (16 bits)` 和 `Block Offset (6 bits)`。**
>
>    **利用 `L2 Cache Index` 定位L2缓存中的特定行，然后通过 `Tag` 来比对是否匹配。**

## Summary 

!! Summary  

- Memory hierarchy
  - From single level to multi level
  - Evaluate the performance parameters of the storage system (average price per bit C; hit rate H; average memory access time T
- Cache basic knowledge
  * Mapping rules
  * Access method
  * Replacement algorithm
  * Write strategy
  * Cache performance analysis
* Virtual Memory (the influence of memory organization structure on Cache failure rate)
